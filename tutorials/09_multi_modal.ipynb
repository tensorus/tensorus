{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 9 (Enhanced): Multi-Modal â€” Cross-Modal (Textâ†”Image, Tensorâ†’Text)\n",
    "\n",
    "We insert text/image/tensor items, build relationships, and run cross-modal searches including tensorâ†’text demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install\n",
    "import sys, subprocess, pkgutil\n",
    "for p in ['numpy','torch','matplotlib','seaborn','requests','pillow']:\n",
    "    if pkgutil.find_loader(p) is None: subprocess.check_call([sys.executable,'-m','pip','install',p])\n",
    "print('âœ… Dependencies ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cef0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_utils import (\n",
    "    ping_server,\n",
    "    ensure_dataset,\n",
    "    ingest_tensor,\n",
    "    fetch_dataset,\n",
    "    summarize_records,\n",
    "    tensor_addition,\n",
    "    pretty_json,\n",
    ")\n",
    "API = \"http://127.0.0.1:7860\"\n",
    "SERVER = ping_server(API)\n",
    "print(f\"ðŸ“¡ Tensorus server available: {SERVER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np, torch, requests\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, List\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "sns.set_theme(style='whitegrid')\n",
    "def embed_text(text:str):\n",
    "    rng=np.random.default_rng(abs(hash(text))%(2**32)); return torch.tensor(rng.normal(size=512), dtype=torch.float32)\n",
    "def embed_image(arr:np.ndarray):\n",
    "    v=torch.tensor(arr.mean(axis=(0,1)), dtype=torch.float32);\n",
    "    return torch.cat([v, torch.zeros(512-v.numel())]) if v.numel()<512 else v\n",
    "def embed_tensor(t:torch.Tensor):\n",
    "    v=t.flatten(); v=v[:512] if v.numel()>512 else torch.cat([v, torch.zeros(512-v.numel())]); return v.float()\n",
    "def cosine(a:torch.Tensor,b:torch.Tensor): a=a.float(); b=b.float(); return float(torch.dot(a,b)/(torch.norm(a)*torch.norm(b)+1e-8))\n",
    "@dataclass\n",
    "class MMItem: data_id:str; modality:str; content:Any; embedding:torch.Tensor; meta:Dict[str,Any]=field(default_factory=dict)\n",
    "store: Dict[str,MMItem]={}; by_mod={'text':[], 'image':[], 'tensor':[]}\n",
    "def random_image(w=128,h=128): return (np.random.rand(h,w,3)*255).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[('doc1','text','CNNs for images',{'domain':'vision'}), ('doc2','text','Speech models with attention',{'domain':'audio'})]\n",
    "for pid,mod,txt,meta in texts:\n",
    "    emb=embed_text(txt); item=MMItem(pid,mod,txt,emb,meta); store[pid]=item; by_mod[mod].append(pid)\n",
    "imgs=[('img1','image', random_image(), {'domain':'vision'}), ('img2','image', random_image(), {'domain':'medical'})]\n",
    "for pid,mod,arr,meta in imgs:\n",
    "    emb=embed_image(arr); item=MMItem(pid,mod,arr,emb,meta); store[pid]=item; by_mod[mod].append(pid)\n",
    "tensors=[('ten1','tensor', torch.randn(64,64), {'domain':'vision'}), ('ten2','tensor', torch.randn(32,128), {'domain':'nlp'})]\n",
    "for pid,mod,t,meta in tensors:\n",
    "    emb=embed_tensor(t); item=MMItem(pid,mod,t,emb,meta); store[pid]=item; by_mod[mod].append(pid)\n",
    "{k:len(v) for k,v in by_mod.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store['doc1'].meta['related']=['img1','ten1']; store['img1'].meta['related']=['doc1']; store['ten1'].meta['related']=['doc1']; 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text â†’ Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q='convolutional networks for images'\n",
    "qe=embed_text(q); cand=[store[i] for i in by_mod['image']]\n",
    "sc=sorted([(c, cosine(qe,c.embedding)) for c in cand], key=lambda x:x[1], reverse=True)[:3]\n",
    "for it,score in sc: print(it.data_id, round(score,3), it.meta.get('domain'))\n",
    "plt.figure(); plt.imshow(sc[0][0].content); plt.title('Best match: '+sc[0][0].data_id); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor â†’ Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qe=store['ten1'].embedding; cand=[store[i] for i in by_mod['text']]\n",
    "sc=sorted([(c, cosine(qe,c.embedding)) for c in cand], key=lambda x:x[1], reverse=True)\n",
    "[(it.data_id, round(s,3)) for it,s in sc]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
