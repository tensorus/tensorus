{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 19: Integration Cookbook (SDK, RAG, LLM)\n",
    "\n",
    "This notebook showcases end-to-end patterns for integrating Tensorus into downstream systems:\n",
    "- Python SDK-style usage via `requests` wrappers\n",
    "- Retrieval-Augmented Generation flow with external LLMs\n",
    "- Prompt assembly that combines Tensorus metadata and vector search results\n",
    "\n",
    "When the Tensorus API is offline, the notebook generates demo responses so you can still inspect workflow code." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_utils import ping_server, ensure_dataset, ingest_tensor, pretty_json\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "API = 'http://127.0.0.1:7860'\n",
    "SERVER = ping_server(API)\n",
    "print(f'\ud83d\udce1 Tensorus server available: {SERVER}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDK-style helper\n",
    "Define a light-weight client class that wraps common API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorusClient:\n",
    "    def __init__(self, api_url: str = API):\n",
    "        self.api_url = api_url.rstrip('/')\n",
    "\n",
    "    def create_dataset(self, name: str):\n",
    "        return requests.post(f"{self.api_url}/datasets/create", json={'name': name})\n",
    "\n",
    "    def ingest(self, dataset: str, tensor: np.ndarray, metadata: dict):\n",
    "        payload = {\n",
    "            'shape': list(tensor.shape),\n",
    "            'dtype': str(tensor.dtype),\n",
    "            'data': tensor.tolist(),\n",
    "            'metadata': metadata,\n",
    "        }\n",
    "        return requests.post(f"{self.api_url}/datasets/{dataset}/ingest", json=payload)\n",
    "\n",
    "    def vector_search(self, dataset: str, query: str, k: int = 5):\n",
    "        payload = {'query': query, 'dataset_name': dataset, 'k': k}\n",
    "        return requests.post(f"{self.api_url}/api/v1/vector/search", json=payload)\n",
    "\n",
    "client = TensorusClient()\n",
    "client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset seeding\n",
    "Seed a dataset with sample passages that an LLM can later use for RAG." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'rag_demo'\n",
    "passages = [\n",
    "    'Tensorus stores multi-dimensional tensors with schema validation.',\n",
    "    'Hybrid search combines semantic and mathematical filters.',\n",
    "    'Vector indexes are backed by FAISS with geometric partitioning.'\n",
    "]\n",
    "\n",
    "if SERVER:\n",
    "    ensure_dataset(DATASET, API)\n",
    "    for idx, text in enumerate(passages):\n",
    "        tensor = np.random.rand(1, 384).astype('float32')\n",
    "        ingest_tensor(DATASET, tensor, {'text': text, 'source_id': idx}, API)\n",
    "    embed_payload = {'texts': passages, 'dataset_name': DATASET}\n",
    "    try:\n",
    "        embed_resp = requests.post(f"{API}/api/v1/vector/embed", json=embed_payload, timeout=30)\n",
    "        embed_resp.raise_for_status()\n",
    "        embed_result = embed_resp.json()\n",
    "    except requests.RequestException as exc:\n",
    "        embed_result = {'error': str(exc), 'demo_mode': True}\n",
    "else:\n",
    "    embed_result = {'demo_mode': True, 'record_ids': [f'demo_{i}' for i in range(len(passages))]}\n",
    "\n",
    "print(pretty_json(embed_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation flow\n",
    "Simulate a question-answering path that retrieves top-k passages then assembles a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'How does Tensorus handle hybrid search?'\n",
    "\n",
    "if SERVER:\n",
    "    try:\n",
    "        search_resp = requests.post(\n",
    "            f"{API}/api/v1/vector/search",\n",
    "            json={'query': question, 'dataset_name': DATASET, 'k': 3},\n",
    "            timeout=15,\n",
    "        )\n",
    "        search_resp.raise_for_status()\n",
    "        search_result = search_resp.json()\n",
    "    except requests.RequestException as exc:\n",
    "        search_result = {'error': str(exc), 'demo_mode': True}\n",
    "else:\n",
    "    search_result = {\n",
    "        'demo_mode': True,\n",
    "        'results': [\n",
    "            {'metadata': {'text': passages[1]}},\n",
    "            {'metadata': {'text': passages[0]}},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "print(pretty_json(search_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt assembly\n",
    "Combine retrieved passages into a structured prompt for an external LLM." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_texts = [r.get('metadata', {}).get('text', 'Demo passage') for r in search_result.get('results', [])]\n",
    "prompt = f"Answer the question using the context below.\nQuestion: {question}\n\nContext:\n- " + "\n- ".join(retrieved_texts)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM call (pseudo-code)\n",
    "Invoke your preferred LLM SDK with the prompt. Below we outline an OpenAI-style call; replace with actual code for your environment." 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_code = '''\nfrom openai import OpenAI\nclient = OpenAI()\nresponse = client.chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[\n        {"role": "system", "content": "You are a helpful analyst."},\n        {"role": "user", "content": prompt},\n    ],\n)\nprint(response.choices[0].message.content)\n'''\nprint(pseudo_code)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
