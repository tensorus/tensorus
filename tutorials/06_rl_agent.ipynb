{
 "cells": [
  {"cell_type":"markdown","metadata":{},"source":["# Tutorial 6 (Enhanced): RL AGENT ‚Äî DQN with Prioritized Replay\n","\n","We build a toy RL environment and train a DQN agent with Prioritized Experience Replay (simulated). We plot rewards/loss and run evaluation."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install\n","import sys, subprocess, pkgutil\n","for p in ['numpy','torch','matplotlib','seaborn','requests']:\n","    if pkgutil.find_loader(p) is None: subprocess.check_call([sys.executable,'-m','pip','install',p])\n","print('‚úÖ Dependencies ready')"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup\n","import math, time, random, numpy as np, torch, requests\n","import torch.nn as nn, torch.optim as optim, torch.nn.functional as F\n","from collections import deque, namedtuple\n","import matplotlib.pyplot as plt, seaborn as sns\n","sns.set_theme(style='whitegrid')\n","API='http://127.0.0.1:7860'\n","def server_ok():\n","    try: return requests.get(f'{API}/health', timeout=2).status_code==200\n","    except: return False\n","SERVER=server_ok(); print('üì° Tensorus:', '‚úÖ Connected' if SERVER else '‚ö†Ô∏è Demo Mode')\n","Experience = namedtuple('Experience','state action reward next_state done')"]},
  {"cell_type":"markdown","metadata":{},"source":["## Environment"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GridEnv:\n","    def __init__(self, size=10): self.size=size; self.reset()\n","    def reset(self): self.agent=[0,0]; self.goal=[self.size-1,self.size-1]; return np.array(self.agent,dtype=np.float32)\n","    def step(self,a):\n","        if a==0: self.agent[1]=min(self.size-1,self.agent[1]+1)\n","        elif a==1: self.agent[1]=max(0,self.agent[1]-1)\n","        elif a==2: self.agent[0]=max(0,self.agent[0]-1)\n","        elif a==3: self.agent[0]=min(self.size-1,self.agent[0]+1)\n","        dist=abs(self.agent[0]-self.goal[0])+abs(self.agent[1]-self.goal[1])\n","        if self.agent==self.goal: return np.array(self.agent,dtype=np.float32),100.0,True,{}\n","        return np.array(self.agent,dtype=np.float32), -0.1-0.01*dist, False, {}\n"]},
  {"cell_type":"markdown","metadata":{},"source":["## Prioritized Replay (simulated) and DQN"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Replay:\n","    def __init__(self, cap=50000): self.cap=cap; self.buf=deque(maxlen=cap); self.pri=deque(maxlen=cap)\n","    def store(self, e:Experience, p=1.0): self.buf.append(e); self.pri.append(max(1e-6,p))\n","    def sample(self, n=64):\n","        n=min(n,len(self.buf)); probs=np.array(self.pri)/sum(self.pri); idx=np.random.choice(len(self.buf), n, p=probs); return [self.buf[i] for i in idx]\n","class QNet(nn.Module):\n","    def __init__(self, sdim=2, adim=4):\n","        super().__init__(); self.net=nn.Sequential(nn.Linear(2,128),nn.ReLU(),nn.Linear(128,128),nn.ReLU(),nn.Linear(128,adim))\n","    def forward(self,x): return self.net(x)\n","class Agent:\n","    def __init__(self): self.q=QNet(); self.tgt=QNet(); self.tgt.load_state_dict(self.q.state_dict()); self.opt=optim.Adam(self.q.parameters(), lr=1e-3); self.gamma=0.99; self.eps=0.2; self.eps_min=0.01; self.eps_decay=0.995; self.step=0\n","    def act(self,s,train=True):\n","        if train and random.random()<self.eps: return random.randrange(4)\n","        with torch.no_grad(): return int(self.q(torch.FloatTensor(s).unsqueeze(0)).argmax().item())\n","    def learn(self,batch):\n","        if not batch: return {'loss':0.0}\n","        S=torch.FloatTensor([e.state for e in batch]); A=torch.LongTensor([e.action for e in batch]); R=torch.FloatTensor([e.reward for e in batch]); NS=torch.FloatTensor([e.next_state for e in batch]); D=torch.BoolTensor([e.done for e in batch])\n","        q=self.q(S).gather(1,A.unsqueeze(1)).squeeze(1)\n","        with torch.no_grad(): tgt=R + self.gamma*self.tgt(NS).max(1)[0]*(~D)\n","        loss=F.mse_loss(q,tgt); self.opt.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(self.q.parameters(),1.0); self.opt.step()\n","        if self.step%100==0: self.tgt.load_state_dict(self.q.state_dict())\n","        self.eps=max(self.eps_min,self.eps*self.eps_decay); self.step+=1; return {'loss':float(loss.item())}\n"]},
  {"cell_type":"markdown","metadata":{},"source":["## Training and Evaluation"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env=GridEnv(10); agent=Agent(); replay=Replay(100000)\n","rewards=[]; losses=[]\n","for ep in range(120):\n","    s=env.reset(); tot=0; ltmp=[]\n","    for t in range(300):\n","        a=agent.act(s,train=True); ns,r,d,_=env.step(a); replay.store(Experience(s,a,r,ns,d), p=1.0+max(0,r))\n","        s=ns; tot+=r; m=agent.learn(replay.sample(64));\n","        if m['loss']>0: ltmp.append(m['loss']);\n","        if d: break\n","    rewards.append(tot); losses.append(np.mean(ltmp) if ltmp else 0.0)\n","plt.figure(figsize=(10,4)); plt.subplot(1,2,1); plt.plot(rewards); plt.title('Rewards'); plt.subplot(1,2,2); plt.plot(losses); plt.title('Loss'); plt.tight_layout(); plt.show()\n","# Evaluation\n","s=env.reset(); tot=0; traj=[tuple(s)]\n","for _ in range(200):\n","    a=agent.act(s,train=False); s,r,d,_=env.step(a); tot+=r; traj.append(tuple(s));\n","    if d: break\n","print('Eval total reward:', round(tot,2),'| steps:', len(traj)-1)"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
