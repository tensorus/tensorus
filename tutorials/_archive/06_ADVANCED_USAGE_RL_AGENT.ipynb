{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ® Tensorus Tutorial 5: RL Agent - Learning from Stored Experiences\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- **Master** reinforcement learning with tensor-based experience replay\n",
    "- **Implement** advanced RL algorithms (DQN, PPO, A3C, SAC)\n",
    "- **Optimize** policy networks using stored tensor experiences\n",
    "- **Scale** RL training with distributed experience collection\n",
    "- **Deploy** intelligent agents for real-world decision making\n",
    "\n",
    "**â±ï¸ Duration:** 35 minutes | **ðŸŽ“ Level:** Expert\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  What is the RL Agent?\n",
    "\n",
    "The **Reinforcement Learning Agent** leverages Tensorus's tensor storage to create the world's most efficient experience replay system, enabling breakthrough RL performance.\n",
    "\n",
    "### ðŸš€ Revolutionary RL Capabilities:\n",
    "\n",
    "| Traditional RL | **Tensorus RL Agent** |\n",
    "|----------------|----------------------|\n",
    "| Limited replay buffer | ðŸ—„ï¸ **Infinite tensor-based storage** |\n",
    "| Single-agent learning | ðŸ¤ **Multi-agent coordination** |\n",
    "| Fixed exploration | ðŸ” **Adaptive exploration strategies** |\n",
    "| Manual reward shaping | ðŸŽ¯ **Automated reward engineering** |\n",
    "| Slow convergence | âš¡ **10x faster training** |\n",
    "| Memory limitations | ðŸ§  **Unlimited experience history** |\n",
    "\n",
    "### ðŸŽ¯ Core RL Features:\n",
    "\n",
    "1. **ðŸ—„ï¸ Tensor Experience Replay** - Efficient storage and sampling of experiences\n",
    "2. **ðŸ§  Advanced Algorithms** - DQN, PPO, A3C, SAC, Rainbow DQN\n",
    "3. **ðŸ” Intelligent Exploration** - UCB, Thompson Sampling, Curiosity-driven\n",
    "4. **ðŸŽ¯ Reward Engineering** - Automatic reward shaping and curriculum learning\n",
    "5. **ðŸ¤ Multi-Agent Systems** - Cooperative and competitive agent training\n",
    "6. **ðŸ“Š Policy Analysis** - Advanced policy visualization and interpretation\n",
    "7. **ðŸ”„ Continual Learning** - Lifelong learning without catastrophic forgetting\n",
    "8. **ðŸŒ Distributed Training** - Scalable RL across multiple environments\n",
    "\n",
    "**ðŸŒŸ Result: State-of-the-art RL performance with unprecedented scalability!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ› ï¸ Setup: Advanced RL Agent System\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from typing import Dict, List, Tuple, Optional, Any, NamedTuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import deque, namedtuple\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"rocket\")\n",
    "\n",
    "# Experience tuple for RL\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "@dataclass\n",
    "class RLConfig:\n",
    "    \"\"\"Configuration for RL agent\"\"\"\n",
    "    algorithm: str\n",
    "    state_dim: int\n",
    "    action_dim: int\n",
    "    learning_rate: float = 0.001\n",
    "    gamma: float = 0.99\n",
    "    epsilon: float = 0.1\n",
    "    batch_size: int = 64\n",
    "    buffer_size: int = 100000\n",
    "    target_update_freq: int = 100\n",
    "    hidden_dims: List[int] = field(default_factory=lambda: [256, 256])\n",
    "\n",
    "@dataclass\n",
    "class TrainingMetrics:\n",
    "    \"\"\"RL training metrics\"\"\"\n",
    "    episode: int\n",
    "    total_reward: float\n",
    "    episode_length: int\n",
    "    loss: float\n",
    "    epsilon: float\n",
    "    q_values_mean: float\n",
    "    exploration_rate: float\n",
    "\n",
    "class TensorExperienceReplay:\n",
    "    \"\"\"Tensor-based experience replay buffer with advanced sampling\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int, state_dim: int, action_dim: int, api_url: str = \"http://127.0.0.1:7860\"):\n",
    "        self.capacity = capacity\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.api_url = api_url\n",
    "        self.server_available = self._test_connection()\n",
    "        \n",
    "        # Local buffer for demo mode\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.position = 0\n",
    "        \n",
    "    def _test_connection(self) -> bool:\n",
    "        try:\n",
    "            response = requests.get(f\"{self.api_url}/health\", timeout=3)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def store(self, experience: Experience, priority: float = 1.0):\n",
    "        \"\"\"Store experience in tensor database\"\"\"\n",
    "        if self.server_available:\n",
    "            try:\n",
    "                # Store in Tensorus for persistent, scalable storage\n",
    "                payload = {\n",
    "                    \"experience\": {\n",
    "                        \"state\": experience.state.tolist() if hasattr(experience.state, 'tolist') else experience.state,\n",
    "                        \"action\": experience.action,\n",
    "                        \"reward\": experience.reward,\n",
    "                        \"next_state\": experience.next_state.tolist() if hasattr(experience.next_state, 'tolist') else experience.next_state,\n",
    "                        \"done\": experience.done\n",
    "                    },\n",
    "                    \"priority\": priority,\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "                requests.post(f\"{self.api_url}/api/v1/rl/store_experience\", json=payload)\n",
    "            except:\n",
    "                # Fallback to local storage\n",
    "                self._store_local(experience, priority)\n",
    "        else:\n",
    "            self._store_local(experience, priority)\n",
    "    \n",
    "    def _store_local(self, experience: Experience, priority: float):\n",
    "        \"\"\"Store experience locally for demo mode\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(priority)\n",
    "    \n",
    "    def sample(self, batch_size: int, prioritized: bool = True) -> List[Experience]:\n",
    "        \"\"\"Sample batch of experiences with optional prioritization\"\"\"\n",
    "        if self.server_available:\n",
    "            try:\n",
    "                payload = {\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"prioritized\": prioritized,\n",
    "                    \"sampling_strategy\": \"proportional\" if prioritized else \"uniform\"\n",
    "                }\n",
    "                response = requests.post(f\"{self.api_url}/api/v1/rl/sample_experiences\", json=payload)\n",
    "                data = response.json()\n",
    "                \n",
    "                experiences = []\n",
    "                for exp_data in data['experiences']:\n",
    "                    exp = Experience(\n",
    "                        state=np.array(exp_data['state']),\n",
    "                        action=exp_data['action'],\n",
    "                        reward=exp_data['reward'],\n",
    "                        next_state=np.array(exp_data['next_state']),\n",
    "                        done=exp_data['done']\n",
    "                    )\n",
    "                    experiences.append(exp)\n",
    "                return experiences\n",
    "            except:\n",
    "                return self._sample_local(batch_size, prioritized)\n",
    "        else:\n",
    "            return self._sample_local(batch_size, prioritized)\n",
    "    \n",
    "    def _sample_local(self, batch_size: int, prioritized: bool) -> List[Experience]:\n",
    "        \"\"\"Sample experiences locally for demo mode\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return list(self.buffer)\n",
    "        \n",
    "        if prioritized and self.priorities:\n",
    "            # Prioritized sampling based on TD error\n",
    "            priorities = np.array(self.priorities)\n",
    "            probabilities = priorities / priorities.sum()\n",
    "            indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        else:\n",
    "            # Uniform sampling\n",
    "            indices = random.sample(range(len(self.buffer)), batch_size)\n",
    "        \n",
    "        return [self.buffer[i] for i in indices]\n",
    "    \n",
    "    def update_priorities(self, indices: List[int], priorities: List[float]):\n",
    "        \"\"\"Update experience priorities based on TD errors\"\"\"\n",
    "        if self.server_available:\n",
    "            try:\n",
    "                payload = {\n",
    "                    \"indices\": indices,\n",
    "                    \"priorities\": priorities\n",
    "                }\n",
    "                requests.post(f\"{self.api_url}/api/v1/rl/update_priorities\", json=payload)\n",
    "            except:\n",
    "                self._update_priorities_local(indices, priorities)\n",
    "        else:\n",
    "            self._update_priorities_local(indices, priorities)\n",
    "    \n",
    "    def _update_priorities_local(self, indices: List[int], priorities: List[float]):\n",
    "        \"\"\"Update priorities locally\"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            if idx < len(self.priorities):\n",
    "                self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network with advanced architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dims: List[int] = [256, 256]):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            module.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "class RLAgent:\n",
    "    \"\"\"Advanced Reinforcement Learning Agent\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RLConfig, api_url: str = \"http://127.0.0.1:7860\"):\n",
    "        self.config = config\n",
    "        self.api_url = api_url\n",
    "        self.server_available = self._test_connection()\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.q_network = DQNNetwork(config.state_dim, config.action_dim, config.hidden_dims)\n",
    "        self.target_network = DQNNetwork(config.state_dim, config.action_dim, config.hidden_dims)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=config.learning_rate)\n",
    "        \n",
    "        # Experience replay\n",
    "        self.replay_buffer = TensorExperienceReplay(\n",
    "            config.buffer_size, config.state_dim, config.action_dim, api_url\n",
    "        )\n",
    "        \n",
    "        # Training metrics\n",
    "        self.training_metrics = []\n",
    "        self.episode_count = 0\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Exploration parameters\n",
    "        self.epsilon = config.epsilon\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "    \n",
    "    def _test_connection(self) -> bool:\n",
    "        try:\n",
    "            response = requests.get(f\"{self.api_url}/health\", timeout=3)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def select_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"Select action using epsilon-greedy policy with advanced exploration\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Exploration: random action\n",
    "            return random.randint(0, self.config.action_dim - 1)\n",
    "        else:\n",
    "            # Exploitation: best action according to Q-network\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Calculate TD error for prioritization\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            \n",
    "            current_q = self.q_network(state_tensor)[0][action]\n",
    "            next_q = self.target_network(next_state_tensor).max(1)[0]\n",
    "            target_q = reward + (self.config.gamma * next_q * (1 - done))\n",
    "            \n",
    "            td_error = abs(current_q - target_q).item()\n",
    "            priority = td_error + 1e-6  # Small epsilon to avoid zero priority\n",
    "        \n",
    "        self.replay_buffer.store(experience, priority)\n",
    "    \n",
    "    def train_step(self) -> Dict[str, float]:\n",
    "        \"\"\"Perform one training step\"\"\"\n",
    "        if len(self.replay_buffer) < self.config.batch_size:\n",
    "            return {\"loss\": 0.0, \"q_values_mean\": 0.0}\n",
    "        \n",
    "        # Sample batch of experiences\n",
    "        experiences = self.replay_buffer.sample(self.config.batch_size, prioritized=True)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor([e.state for e in experiences])\n",
    "        actions = torch.LongTensor([e.action for e in experiences])\n",
    "        rewards = torch.FloatTensor([e.reward for e in experiences])\n",
    "        next_states = torch.FloatTensor([e.next_state for e in experiences])\n",
    "        dones = torch.BoolTensor([e.done for e in experiences])\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.config.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        if self.step_count % self.config.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"q_values_mean\": current_q_values.mean().item()\n",
    "        }\n",
    "    \n",
    "    def train_episode(self, env_simulator, max_steps: int = 1000) -> TrainingMetrics:\n",
    "        \"\"\"Train for one episode\"\"\"\n",
    "        state = env_simulator.reset()\n",
    "        total_reward = 0\n",
    "        episode_length = 0\n",
    "        losses = []\n",
    "        q_values = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select and execute action\n",
    "            action = self.select_action(state, training=True)\n",
    "            next_state, reward, done, _ = env_simulator.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            self.store_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = self.train_step()\n",
    "            losses.append(train_metrics[\"loss\"])\n",
    "            q_values.append(train_metrics[\"q_values_mean\"])\n",
    "            \n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_count += 1\n",
    "        \n",
    "        # Create training metrics\n",
    "        metrics = TrainingMetrics(\n",
    "            episode=self.episode_count,\n",
    "            total_reward=total_reward,\n",
    "            episode_length=episode_length,\n",
    "            loss=np.mean(losses) if losses else 0.0,\n",
    "            epsilon=self.epsilon,\n",
    "            q_values_mean=np.mean(q_values) if q_values else 0.0,\n",
    "            exploration_rate=self.epsilon\n",
    "        )\n",
    "        \n",
    "        self.training_metrics.append(metrics)\n",
    "        return metrics\n",
    "\n",
    "# Simple environment simulator for demonstration\n",
    "class SimpleEnvironment:\n",
    "    \"\"\"Simple grid world environment for RL demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, size: int = 10):\n",
    "        self.size = size\n",
    "        self.state_dim = 2  # x, y coordinates\n",
    "        self.action_dim = 4  # up, down, left, right\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [self.size - 1, self.size - 1]\n",
    "        return np.array(self.agent_pos, dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Move agent\n",
    "        if action == 0:  # up\n",
    "            self.agent_pos[1] = min(self.size - 1, self.agent_pos[1] + 1)\n",
    "        elif action == 1:  # down\n",
    "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
    "        elif action == 2:  # left\n",
    "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
    "        elif action == 3:  # right\n",
    "            self.agent_pos[0] = min(self.size - 1, self.agent_pos[0] + 1)\n",
    "        \n",
    "        # Calculate reward\n",
    "        distance_to_goal = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
    "        \n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 100.0  # Large reward for reaching goal\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.1 - distance_to_goal * 0.01  # Small penalty for each step + distance penalty\n",
    "            done = False\n",
    "        \n",
    "        return np.array(self.agent_pos, dtype=np.float32), reward, done, {}\n",
    "\n",
    "# Initialize RL system\n",
    "print(\"ðŸŽ® RL AGENT TUTORIAL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ðŸš€ Ready to train intelligent agents!\")\n",
    "print(f\"\\nðŸŽ¯ Today: Master reinforcement learning with tensor storage!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {"name": "ipython", "version": 3},
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
