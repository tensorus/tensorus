{
 "cells": [
  {"cell_type":"markdown","metadata":{},"source":["# Tutorial 9: Multi-Modal ‚Äî Text, Images, and Tensors Together\n","\n","Insert and link text, images, and tensors. Perform a cross-modal search (e.g., text ‚Üí images). Runs in Connected Mode (Tensorus API) or Demo Mode (local)."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lightweight install cell\n","import sys, subprocess, pkgutil\n","for p in ['numpy','torch','matplotlib','seaborn','requests','pillow']:\n","    if pkgutil.find_loader(p) is None:\n","        subprocess.check_call([sys.executable,'-m','pip','install',p])\n","print('‚úÖ Dependencies ready')"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],,"source":["# Setup\n","import io, base64, time, json, random, requests, numpy as np, torch\n","from dataclasses import dataclass, field\n","from typing import Dict, List, Any, Optional, Tuple\n","from PIL import Image\n","import matplotlib.pyplot as plt, seaborn as sns\n","sns.set_theme(style='whitegrid')\n","API='http://127.0.0.1:7860'\n","def server_ok():\n","    try: return requests.get(f'{API}/health', timeout=2).status_code==200\n","    except: return False\n","SERVER=server_ok(); print('üì° Tensorus:', '‚úÖ Connected' if SERVER else '‚ö†Ô∏è Demo Mode')\n","\n","def embed_text(text:str):\n","    rng=np.random.default_rng(abs(hash(text))%(2**32)); return torch.tensor(rng.normal(size=512), dtype=torch.float32)\n","def embed_image(arr:np.ndarray):\n","    v=torch.tensor(arr.mean(axis=(0,1)), dtype=torch.float32)\n","    if v.numel()<512: v=torch.cat([v, torch.zeros(512-v.numel())])\n","    return v\n","def embed_tensor(t:torch.Tensor):\n","    v=t.flatten(); v=v[:512] if v.numel()>512 else torch.cat([v, torch.zeros(512-v.numel())]); return v.float()\n","def cosine(a:torch.Tensor,b:torch.Tensor):\n","    a=a.float(); b=b.float(); return float(torch.dot(a,b)/(torch.norm(a)*torch.norm(b)+1e-8))\n","\n","@dataclass\n","class MMItem:\n","    data_id:str; modality:str; content:Any; embedding:torch.Tensor; meta:Dict[str,Any]=field(default_factory=dict); tags:List[str]=field(default_factory=list)\n","\n","store: Dict[str,MMItem]={}\n","by_modality={'text':[], 'image':[], 'tensor':[]}\n"]},
  {"cell_type":"markdown","metadata":{},"source":["## Step 1 ‚Äî Insert Multi-Modal Items"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],,"source":["# Texts\n","texts=[\n","  ('doc_neural_1','text','A research note about convolutional neural networks for image classification',{'domain':'vision'}),\n","  ('doc_audio_1','text','An article about speech recognition with transformers and self-attention',{'domain':'audio'})\n","]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],,"source":["for pid,mod,txt,meta in texts:\n","    emb=embed_text(txt)\n","    item=MMItem(pid,mod,txt,emb,meta,['example'])\n","    store[pid]=item; by_modality[mod].append(pid)\n","print('Inserted text items:', by_modality['text'])"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],,"source":["# Images (random demo images)\n","def random_image(w=128,h=128):\n","    arr=(np.random.rand(h,w,3)*255).astype('uint8'); return arr\n","imgs=[('img_city','image', random_image(), {'domain':'vision'}), ('img_brain','image', random_image(), {'domain':'medical'})]\n","for pid,mod,arr,meta in imgs:\n","    emb=embed_image(arr)\n","    item=MMItem(pid,mod,arr,emb,meta,['example'])\n","    store[pid]=item; by_modality[mod].append(pid)\n","print('Inserted image items:', by_modality['image'])"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],,"source":["# Tensors\n","tensors=[('tensor_weights','tensor', torch.randn(64,64), {'domain':'vision'}), ('tensor_feats','tensor', torch.randn(32,128), {'domain':'nlp'})]\n","for pid,mod,t,meta in tensors:\n","    emb=embed_tensor(t)\n","    item=MMItem(pid,mod,t,emb,meta,['example'])\n","    store[pid]=item; by_modality[mod].append(pid)\n","print('Inserted tensor items:', by_modality['tensor'])"]},
  {"cell_type":"markdown","metadata":{},"source":["## Step 2 ‚Äî Create Relationships (Bidirectional)"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],,"source":["links={ 'doc_neural_1': ['img_city','tensor_weights'], 'doc_audio_1': [] }\n","for a, bs in links.items():\n","    store[a].meta['related']=bs\n","    for b in bs:\n","        store[b].meta['related']=store[b].meta.get('related', []) + [a]\n","print('Relationships set')"]},
  {"cell_type":"markdown","metadata":{},"source":["## Step 3 ‚Äî Cross-Modal Search (Text ‚Üí Images)"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],,"source":["query_text='convolutional neural networks for images'\n","q_emb=embed_text(query_text)\n","candidates=[store[i] for i in by_modality['image']]\n","scored=[(c, cosine(q_emb, c.embedding)) for c in candidates]\n","scored.sort(key=lambda x: x[1], reverse=True)\n","top=scored[:3]\n","for item,score in top:\n","    print(item.data_id, '| similarity:', round(score,3), '| domain:', item.meta.get('domain'))\n","if top:\n","    best=top[0][0]\n","    plt.figure(); plt.imshow(best.content); plt.title('Best match: '+best.data_id); plt.axis('off'); plt.show()"]},
  {"cell_type":"markdown","metadata":{},"source":["## Step 4 ‚Äî System Summary"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],,"source":["summary={ 'total': len(store), 'by_modality': {k: len(v) for k,v in by_modality.items()} }\n","summary"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
