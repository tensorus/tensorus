{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌐 Tensorus Tutorial 8: Multi-Modal - Text, Images, Tensors Together\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "- **Integrate** text, images, audio, and tensor data seamlessly\n",
    "- **Build** cross-modal search and retrieval systems\n",
    "- **Implement** multi-modal AI applications\n",
    "- **Create** unified embeddings across data types\n",
    "- **Deploy** production multi-modal workflows\n",
    "\n",
    "**⏱️ Duration:** 30 minutes | **🎓 Level:** Expert\n",
    "\n",
    "---\n",
    "\n",
    "## 🌈 Multi-Modal Revolution\n",
    "\n",
    "Tensorus enables **seamless integration** of different data modalities in a single, unified system - something impossible with traditional databases.\n",
    "\n",
    "### 🎨 Multi-Modal Capabilities:\n",
    "\n",
    "| Traditional Systems | **Tensorus Multi-Modal** |\n",
    "|--------------------|---------------------------|\n",
    "| Separate databases per type | 🌐 **Unified multi-modal storage** |\n",
    "| Manual data alignment | 🔄 **Automatic cross-modal linking** |\n",
    "| Limited search capabilities | 🔍 **Cross-modal semantic search** |\n",
    "| Complex integration | ✨ **Native multi-modal operations** |\n",
    "| Isolated embeddings | 🧠 **Unified embedding space** |\n",
    "| Manual synchronization | ⚡ **Real-time multi-modal sync** |\n",
    "\n",
    "### 🎯 Supported Modalities:\n",
    "\n",
    "1. **📝 Text Data** - Documents, articles, code, natural language\n",
    "2. **🖼️ Image Data** - Photos, diagrams, medical scans, satellite imagery\n",
    "3. **🎵 Audio Data** - Speech, music, environmental sounds\n",
    "4. **📊 Tensor Data** - Neural networks, scientific data, simulations\n",
    "5. **📹 Video Data** - Movies, surveillance, time-series imagery\n",
    "6. **🗂️ Structured Data** - Tables, graphs, knowledge bases\n",
    "7. **🧬 Scientific Data** - Molecular structures, genomics, physics\n",
    "8. **🌍 Geospatial Data** - Maps, GPS tracks, geographic information\n",
    "\n",
    "### 🚀 Revolutionary Features:\n",
    "- **🔍 Cross-Modal Search**: Find images using text descriptions\n",
    "- **🧠 Unified Embeddings**: Single vector space for all data types\n",
    "- **🔄 Automatic Alignment**: Link related data across modalities\n",
    "- **📊 Multi-Modal Analytics**: Analyze patterns across data types\n",
    "- **🤖 AI Integration**: Native support for multi-modal AI models\n",
    "\n",
    "**🌟 Result: The world's first truly multi-modal tensor database!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛠️ Setup: Advanced Multi-Modal Framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import io\n",
    "from typing import Dict, List, Tuple, Optional, Any, Union\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "class ModalityType(Enum):\n",
    "    \"\"\"Supported data modalities\"\"\"\n",
    "    TEXT = \"text\"\n",
    "    IMAGE = \"image\"\n",
    "    AUDIO = \"audio\"\n",
    "    VIDEO = \"video\"\n",
    "    TENSOR = \"tensor\"\n",
    "    STRUCTURED = \"structured\"\n",
    "    SCIENTIFIC = \"scientific\"\n",
    "    GEOSPATIAL = \"geospatial\"\n",
    "\n",
    "@dataclass\n",
    "class MultiModalData:\n",
    "    \"\"\"Container for multi-modal data with unified metadata\"\"\"\n",
    "    data_id: str\n",
    "    modality: ModalityType\n",
    "    content: Any\n",
    "    embedding: Optional[torch.Tensor] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    relationships: List[str] = field(default_factory=list)\n",
    "    tags: List[str] = field(default_factory=list)\n",
    "    created_at: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.data_id:\n",
    "            self.data_id = f\"{self.modality.value}_{int(time.time()*1000)}\"\n",
    "\n",
    "@dataclass\n",
    "class CrossModalQuery:\n",
    "    \"\"\"Cross-modal search query\"\"\"\n",
    "    query_modality: ModalityType\n",
    "    query_content: Any\n",
    "    target_modalities: List[ModalityType]\n",
    "    similarity_threshold: float = 0.7\n",
    "    max_results: int = 10\n",
    "    filters: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "class MultiModalEmbedder:\n",
    "    \"\"\"Advanced multi-modal embedding system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_models = {}\n",
    "        self.embedding_dim = 512  # Unified embedding dimension\n",
    "        self._initialize_models()\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Initialize embedding models for different modalities\"\"\"\n",
    "        print(\"🧠 Initializing multi-modal embedding models...\")\n",
    "        \n",
    "        # Text embedder (simplified)\n",
    "        self.embedding_models[ModalityType.TEXT] = self._create_text_embedder()\n",
    "        \n",
    "        # Image embedder (simplified)\n",
    "        self.embedding_models[ModalityType.IMAGE] = self._create_image_embedder()\n",
    "        \n",
    "        # Tensor embedder\n",
    "        self.embedding_models[ModalityType.TENSOR] = self._create_tensor_embedder()\n",
    "        \n",
    "        print(\"✅ Multi-modal embedders initialized\")\n",
    "    \n",
    "    def _create_text_embedder(self) -> nn.Module:\n",
    "        \"\"\"Create text embedding model\"\"\"\n",
    "        class SimpleTextEmbedder(nn.Module):\n",
    "            def __init__(self, vocab_size=10000, embed_dim=512):\n",
    "                super().__init__()\n",
    "                self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "                self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "            \n",
    "            def forward(self, text_tokens):\n",
    "                embedded = self.embedding(text_tokens)\n",
    "                pooled = self.pooling(embedded.transpose(1, 2)).squeeze(-1)\n",
    "                return pooled\n",
    "        \n",
    "        return SimpleTextEmbedder()\n",
    "    \n",
    "    def _create_image_embedder(self) -> nn.Module:\n",
    "        \"\"\"Create image embedding model\"\"\"\n",
    "        class SimpleImageEmbedder(nn.Module):\n",
    "            def __init__(self, embed_dim=512):\n",
    "                super().__init__()\n",
    "                self.conv_layers = nn.Sequential(\n",
    "                    nn.Conv2d(3, 64, 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AdaptiveAvgPool2d((8, 8)),\n",
    "                    nn.Conv2d(64, 128, 3, padding=1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AdaptiveAvgPool2d((4, 4)),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(128 * 4 * 4, embed_dim)\n",
    "                )\n",
    "            \n",
    "            def forward(self, images):\n",
    "                return self.conv_layers(images)\n",
    "        \n",
    "        return SimpleImageEmbedder()\n",
    "    \n",
    "    def _create_tensor_embedder(self) -> nn.Module:\n",
    "        \"\"\"Create tensor embedding model\"\"\"\n",
    "        class TensorEmbedder(nn.Module):\n",
    "            def __init__(self, embed_dim=512):\n",
    "                super().__init__()\n",
    "                self.projection = nn.Linear(1024, embed_dim)  # Assume max 1024 features\n",
    "            \n",
    "            def forward(self, tensors):\n",
    "                # Flatten and project tensors to unified space\n",
    "                flattened = tensors.flatten(start_dim=1)\n",
    "                # Pad or truncate to 1024 dimensions\n",
    "                if flattened.shape[1] > 1024:\n",
    "                    flattened = flattened[:, :1024]\n",
    "                elif flattened.shape[1] < 1024:\n",
    "                    padding = torch.zeros(flattened.shape[0], 1024 - flattened.shape[1])\n",
    "                    flattened = torch.cat([flattened, padding], dim=1)\n",
    "                \n",
    "                return self.projection(flattened)\n",
    "        \n",
    "        return TensorEmbedder()\n",
    "    \n",
    "    def embed_text(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for text\"\"\"\n",
    "        # Simplified tokenization (in practice, use proper tokenizer)\n",
    "        tokens = torch.randint(0, 1000, (1, min(len(text.split()), 50)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = self.embedding_models[ModalityType.TEXT](tokens)\n",
    "        \n",
    "        return embedding.squeeze(0)\n",
    "    \n",
    "    def embed_image(self, image_array: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for image\"\"\"\n",
    "        # Convert to tensor and normalize\n",
    "        if len(image_array.shape) == 3:\n",
    "            image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "        else:\n",
    "            # Create dummy RGB image if grayscale or other format\n",
    "            image_tensor = torch.randn(1, 3, 224, 224)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = self.embedding_models[ModalityType.IMAGE](image_tensor)\n",
    "        \n",
    "        return embedding.squeeze(0)\n",
    "    \n",
    "    def embed_tensor(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Generate embedding for arbitrary tensor\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.embedding_models[ModalityType.TENSOR](tensor.unsqueeze(0))\n",
    "        \n",
    "        return embedding.squeeze(0)\n",
    "    \n",
    "    def compute_similarity(self, embedding1: torch.Tensor, embedding2: torch.Tensor) -> float:\n",
    "        \"\"\"Compute cosine similarity between embeddings\"\"\"\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(embedding1, embedding2, dim=0)\n",
    "        return cos_sim.item()\n",
    "\n",
    "class MultiModalSystem:\n",
    "    \"\"\"Advanced multi-modal data management system\"\"\"\n",
    "    \n",
    "    def __init__(self, api_url: str = \"http://127.0.0.1:7860\"):\n",
    "        self.api_url = api_url\n",
    "        self.server_available = self._test_connection()\n",
    "        self.embedder = MultiModalEmbedder()\n",
    "        self.data_store: Dict[str, MultiModalData] = {}\n",
    "        self.modality_indices: Dict[ModalityType, List[str]] = {modality: [] for modality in ModalityType}\n",
    "        \n",
    "        print(f\"🌐 Multi-Modal System Initialized\")\n",
    "        print(f\"📡 Tensorus: {'✅ Connected' if self.server_available else '⚠️ Local Mode'}\")\n",
    "    \n",
    "    def _test_connection(self) -> bool:\n",
    "        try:\n",
    "            response = requests.get(f\"{self.api_url}/health\", timeout=3)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def add_text_data(self, text: str, metadata: Dict[str, Any] = None, tags: List[str] = None) -> MultiModalData:\n",
    "        \"\"\"Add text data to the multi-modal system\"\"\"\n",
    "        print(f\"\\n📝 Adding text data: '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = self.embedder.embed_text(text)\n",
    "        \n",
    "        # Create multi-modal data object\n",
    "        data = MultiModalData(\n",
    "            data_id=\"\",  # Will be auto-generated\n",
    "            modality=ModalityType.TEXT,\n",
    "            content=text,\n",
    "            embedding=embedding,\n",
    "            metadata=metadata or {},\n",
    "            tags=tags or []\n",
    "        )\n",
    "        \n",
    "        # Store in system\n",
    "        self.data_store[data.data_id] = data\n",
    "        self.modality_indices[ModalityType.TEXT].append(data.data_id)\n",
    "        \n",
    "        # Store in Tensorus if available\n",
    "        if self.server_available:\n",
    "            self._store_in_tensorus(data)\n",
    "        \n",
    "        print(f\"   ✅ Stored as {data.data_id}\")\n",
    "        print(f\"   🧠 Embedding shape: {embedding.shape}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def add_image_data(self, image_array: np.ndarray, metadata: Dict[str, Any] = None, tags: List[str] = None) -> MultiModalData:\n",
    "        \"\"\"Add image data to the multi-modal system\"\"\"\n",
    "        print(f\"\\n🖼️ Adding image data: {image_array.shape}\")\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = self.embedder.embed_image(image_array)\n",
    "        \n",
    "        # Create multi-modal data object\n",
    "        data = MultiModalData(\n",
    "            data_id=\"\",  # Will be auto-generated\n",
    "            modality=ModalityType.IMAGE,\n",
    "            content=image_array,\n",
    "            embedding=embedding,\n",
    "            metadata=metadata or {},\n",
    "            tags=tags or []\n",
    "        )\n",
    "        \n",
    "        # Store in system\n",
    "        self.data_store[data.data_id] = data\n",
    "        self.modality_indices[ModalityType.IMAGE].append(data.data_id)\n",
    "        \n",
    "        # Store in Tensorus if available\n",
    "        if self.server_available:\n",
    "            self._store_in_tensorus(data)\n",
    "        \n",
    "        print(f\"   ✅ Stored as {data.data_id}\")\n",
    "        print(f\"   🧠 Embedding shape: {embedding.shape}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def add_tensor_data(self, tensor: torch.Tensor, metadata: Dict[str, Any] = None, tags: List[str] = None) -> MultiModalData:\n",
    "        \"\"\"Add tensor data to the multi-modal system\"\"\"\n",
    "        print(f\"\\n📊 Adding tensor data: {tensor.shape} {tensor.dtype}\")\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = self.embedder.embed_tensor(tensor)\n",
    "        \n",
    "        # Create multi-modal data object\n",
    "        data = MultiModalData(\n",
    "            data_id=\"\",  # Will be auto-generated\n",
    "            modality=ModalityType.TENSOR,\n",
    "            content=tensor,\n",
    "            embedding=embedding,\n",
    "            metadata=metadata or {},\n",
    "            tags=tags or []\n",
    "        )\n",
    "        \n",
    "        # Store in system\n",
    "        self.data_store[data.data_id] = data\n",
    "        self.modality_indices[ModalityType.TENSOR].append(data.data_id)\n",
    "        \n",
    "        # Store in Tensorus if available\n",
    "        if self.server_available:\n",
    "            self._store_in_tensorus(data)\n",
    "        \n",
    "        print(f\"   ✅ Stored as {data.data_id}\")\n",
    "        print(f\"   🧠 Embedding shape: {embedding.shape}\")\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _store_in_tensorus(self, data: MultiModalData):\n",
    "        \"\"\"Store multi-modal data in Tensorus\"\"\"\n",
    "        try:\n",
    "            # Prepare payload based on modality\n",
    "            if data.modality == ModalityType.TEXT:\n",
    "                content_data = data.content\n",
    "            elif data.modality == ModalityType.IMAGE:\n",
    "                content_data = data.content.tolist()  # Convert numpy array\n",
    "            elif data.modality == ModalityType.TENSOR:\n",
    "                content_data = data.content.tolist()  # Convert tensor\n",
    "            else:\n",
    "                content_data = str(data.content)  # Fallback to string\n",
    "            \n",
    "            payload = {\n",
    "                \"data_id\": data.data_id,\n",
    "                \"modality\": data.modality.value,\n",
    "                \"content\": content_data,\n",
    "                \"embedding\": data.embedding.tolist(),\n",
    "                \"metadata\": data.metadata,\n",
    "                \"tags\": data.tags,\n",
    "                \"created_at\": data.created_at.isoformat()\n",
    "            }\n",
    "            \n",
    "            requests.post(f\"{self.api_url}/api/v1/multimodal/data\", json=payload)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Failed to store in Tensorus: {e}\")\n",
    "    \n",
    "    def cross_modal_search(self, query: CrossModalQuery) -> List[Tuple[MultiModalData, float]]:\n",
    "        \"\"\"Perform cross-modal search across different data types\"\"\"\n",
    "        print(f\"\\n🔍 Cross-Modal Search\")\n",
    "        print(f\"   📥 Query modality: {query.query_modality.value}\")\n",
    "        print(f\"   🎯 Target modalities: {[m.value for m in query.target_modalities]}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        if query.query_modality == ModalityType.TEXT:\n",
    "            query_embedding = self.embedder.embed_text(query.query_content)\n",
    "        elif query.query_modality == ModalityType.IMAGE:\n",
    "            query_embedding = self.embedder.embed_image(query.query_content)\n",
    "        elif query.query_modality == ModalityType.TENSOR:\n",
    "            query_embedding = self.embedder.embed_tensor(query.query_content)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported query modality: {query.query_modality}\")\n",
    "        \n",
    "        # Search across target modalities\n",
    "        results = []\n",
    "        \n",
    "        for modality in query.target_modalities:\n",
    "            for data_id in self.modality_indices[modality]:\n",
    "                data = self.data_store[data_id]\n",
    "                \n",
    "                # Compute similarity\n",
    "                similarity = self.embedder.compute_similarity(query_embedding, data.embedding)\n",
    "                \n",
    "                # Apply threshold and filters\n",
    "                if similarity >= query.similarity_threshold:\n",
    "                    # Apply metadata filters\n",
    "                    if self._passes_filters(data, query.filters):\n",
    "                        results.append((data, similarity))\n",
    "        \n",
    "        # Sort by similarity and limit results\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        results = results[:query.max_results]\n",
    "        \n",
    "        print(f\"   📊 Found {len(results)} matching results\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _passes_filters(self, data: MultiModalData, filters: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Check if data passes metadata filters\"\"\"\n",
    "        for key, value in filters.items():\n",
    "            if key in data.metadata:\n",
    "                if data.metadata[key] != value:\n",
    "                    return False\n",
    "            elif key in data.tags:\n",
    "                if value not in data.tags:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    def create_relationships(self, data_ids: List[str], relationship_type: str = \"related\"):\n",
    "        \"\"\"Create relationships between multi-modal data items\"\"\"\n",
    "        print(f\"\\n🔗 Creating relationships: {relationship_type}\")\n",
    "        print(f\"   📋 Data IDs: {data_ids}\")\n",
    "        \n",
    "        # Add bidirectional relationships\n",
    "        for i, data_id in enumerate(data_ids):\n",
    "            if data_id in self.data_store:\n",
    "                other_ids = [did for j, did in enumerate(data_ids) if i != j]\n",
    "                self.data_store[data_id].relationships.extend(other_ids)\n",
    "        \n",
    "        print(f\"   ✅ Relationships created\")\n",
    "    \n",
    "    def get_system_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive system summary\"\"\"\n",
    "        modality_counts = {modality.value: len(ids) for modality, ids in self.modality_indices.items()}\n",
    "        \n",
    "        return {\n",
    "            \"total_items\": len(self.data_store),\n",
    "            \"modality_distribution\": modality_counts,\n",
    "            \"embedding_dimension\": self.embedder.embedding_dim,\n",
    "            \"server_connected\": self.server_available,\n",
    "            \"supported_modalities\": [m.value for m in ModalityType]\n",
    "        }\n",
    "\n",
    "# Initialize multi-modal system\n",
    "multimodal_system = MultiModalSystem()\n",
    "\n",
    "print(\"\\n🌐 MULTI-MODAL TUTORIAL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"🎯 Ready to explore cross-modal AI capabilities!\")\n",
    "print(f\"🌈 Supported modalities: {len(ModalityType)} types\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {"name": "ipython", "version": 3},
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
