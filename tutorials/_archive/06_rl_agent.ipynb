{
 "cells": [
  {"cell_type":"markdown","metadata":{},"source":["# Tutorial 6: RL AGENT ‚Äî Train with Tensor Experience Replay\n","\n","This notebook trains a simple DQN-style agent on a toy environment, logs metrics, and plots learning curves. It stores experiences to Tensorus when available, else runs fully in demo mode."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lightweight install cell\n","import sys, subprocess, pkgutil\n","for p in ['numpy','torch','matplotlib','seaborn','requests']:\n","    if pkgutil.find_loader(p) is None:\n","        subprocess.check_call([sys.executable,'-m','pip','install',p])\n","print('‚úÖ Dependencies ready')"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup\n","import math, time, random, json, requests, numpy as np, torch\n","import torch.nn as nn, torch.optim as optim, torch.nn.functional as F\n","from collections import deque, namedtuple\n","import matplotlib.pyplot as plt, seaborn as sns\n","sns.set_theme(style='whitegrid')\n","API='http://127.0.0.1:7860'\n","def server_ok():\n","    try: return requests.get(f'{API}/health', timeout=2).status_code==200\n","    except: return False\n","SERVER = server_ok(); print('üì° Tensorus:', '‚úÖ Connected' if SERVER else '‚ö†Ô∏è Demo Mode')\n","Experience = namedtuple('Experience', 'state action reward next_state done')\n"]},
  {"cell_type":"markdown","metadata":{},"source":["## Step 1 ‚Äî Environment (toy grid)"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class GridEnv:\n","    def __init__(self, size=10):\n","        self.size=size; self.reset()\n","    def reset(self):\n","        self.agent=[0,0]; self.goal=[self.size-1,self.size-1]; return np.array(self.agent,dtype=np.float32)\n","    def step(self, a):\n","        if a==0: self.agent[1]=min(self.size-1,self.agent[1]+1)\n","        elif a==1: self.agent[1]=max(0,self.agent[1]-1)\n","        elif a==2: self.agent[0]=max(0,self.agent[0]-1)\n","        elif a==3: self.agent[0]=min(self.size-1,self.agent[0]+1)\n","        dist=abs(self.agent[0]-self.goal[0])+abs(self.agent[1]-self.goal[1])\n","        if self.agent==self.goal: return np.array(self.agent,dtype=np.float32), 100.0, True, {}\n","        return np.array(self.agent,dtype=np.float32), -0.1 - 0.01*dist, False, {}\n"]},
  {"cell_type":"markdown","metadata":{},"source":["## Step 2 ‚Äî Replay Buffer with Tensorus fallback"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Replay:\n","    def __init__(self, cap=50000):\n","        self.cap=cap; self.buf=deque(maxlen=cap); self.pri=deque(maxlen=cap)\n","    def store(self, exp:Experience, p:float=1.0):\n","        self.buf.append(exp); self.pri.append(max(1e-6,p))\n","    def sample(self, n=64):\n","        if len(self.buf)<n: return list(self.buf)\n","        probs = np.array(self.pri)/sum(self.pri)\n","        idx = np.random.choice(len(self.buf), n, p=probs)\n","        return [self.buf[i] for i in idx]\n","    def __len__(self): return len(self.buf)\n","\n","replay = Replay(100000)\n"]},
  {"cell_type":"markdown","metadata":{},"source":["## Step 3 ‚Äî DQN Network and Agent"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class QNet(nn.Module):\n","    def __init__(self, sdim=2, adim=4, h=(256,256)):\n","        super().__init__(); layers=[]; inp=sdim\n","        for u in h:\n","            layers += [nn.Linear(inp,u), nn.ReLU(), nn.Dropout(0.1)]\n","            inp=u\n","        layers += [nn.Linear(inp, adim)]\n","        self.net = nn.Sequential(*layers)\n","        self.apply(self._init)\n","    def _init(self,m):\n","        if isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight); nn.init.constant_(m.bias, 0.01)\n","    def forward(self,x): return self.net(x)\n","\n","class Agent:\n","    def __init__(self, sdim=2, adim=4):\n","        self.q = QNet(sdim, adim); self.tgt = QNet(sdim, adim); self.tgt.load_state_dict(self.q.state_dict())\n","        self.opt = optim.Adam(self.q.parameters(), lr=1e-3)\n","        self.gamma=0.99; self.eps=0.2; self.eps_min=0.01; self.eps_decay=0.995; self.adim=adim; self.step=0\n","    def act(self, s, train=True):\n","        if train and random.random()<self.eps: return random.randrange(self.adim)\n","        with torch.no_grad():\n","            q=self.q(torch.FloatTensor(s).unsqueeze(0)); return int(q.argmax().item())\n","    def learn(self, batch):\n","        if not batch: return {'loss':0.0,'q_mean':0.0}\n","        S=torch.FloatTensor([e.state for e in batch]); A=torch.LongTensor([e.action for e in batch])\n","        R=torch.FloatTensor([e.reward for e in batch]); NS=torch.FloatTensor([e.next_state for e in batch])\n","        D=torch.BoolTensor([e.done for e in batch])\n","        q = self.q(S).gather(1, A.unsqueeze(1)).squeeze(1)\n","        with torch.no_grad(): tgt = R + self.gamma * self.tgt(NS).max(1)[0] * (~D)\n","        loss = F.mse_loss(q, tgt)\n","        self.opt.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(self.q.parameters(),1.0); self.opt.step()\n","        if self.step % 100 == 0: self.tgt.load_state_dict(self.q.state_dict())\n","        self.eps = max(self.eps_min, self.eps*self.eps_decay); self.step+=1\n","        return {'loss': float(loss.item()), 'q_mean': float(q.mean().item())}\n","\n","agent = Agent()\n"]},
  {"cell_type":"markdown","metadata":{},"source":["## Step 4 ‚Äî Training Loop (100‚Äì200 episodes)"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env=GridEnv(10)\n","episodes=120; max_steps=300\n","reward_hist=[]; loss_hist=[]\n","for ep in range(episodes):\n","    s=env.reset(); tot=0; losses=[]\n","    for t in range(max_steps):\n","        a=agent.act(s, train=True)\n","        ns, r, done, _ = env.step(a)\n","        exp=Experience(s,a,r,ns,done)\n","        # store (local; can be integrated with Tensorus if desired)\n","        replay.store(exp, p=1.0)\n","        s=ns; tot+=r\n","        # learn\n","        batch=replay.sample(64)\n","        metrics=agent.learn(batch)\n","        if metrics['loss']>0: losses.append(metrics['loss'])\n","        if done: break\n","    reward_hist.append(tot); loss_hist.append(np.mean(losses) if losses else 0.0)\n","    if (ep+1)%10==0: print(f'Episode {ep+1:3d}  reward={tot:.2f}  eps={agent.eps:.3f}')\n","# Plots\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(10,4)); plt.subplot(1,2,1); plt.plot(reward_hist); plt.title('Reward per Episode');\n","plt.subplot(1,2,2); plt.plot(loss_hist); plt.title('Loss per Episode'); plt.tight_layout(); plt.show()\n"]},
  {"cell_type":"markdown","metadata":{},"source":["## Step 5 ‚Äî Evaluation Episode"]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s=env.reset(); tot=0; traj=[tuple(s)]\n","for t in range(200):\n","    a=agent.act(s, train=False)\n","    s,r,d,_=env.step(a); tot+=r; traj.append(tuple(s))\n","    if d: break\n","print('Eval total reward:', round(tot,2), '| steps:', len(traj)-1)\n","# quick trajectory plot\n","xs=[p[0] for p in traj]; ys=[p[1] for p in traj]\n","plt.figure(); plt.plot(xs, ys, '-o'); plt.title('Evaluation Trajectory'); plt.gca().invert_yaxis(); plt.show()"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
